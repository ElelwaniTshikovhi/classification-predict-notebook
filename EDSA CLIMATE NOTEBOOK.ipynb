{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d2b84c",
   "metadata": {},
   "source": [
    "# CLASSIFICATION Predict Student Solution\n",
    "\n",
    "© Explore Data Science Academy\n",
    "\n",
    "---\n",
    "### Honour Code\n",
    "\n",
    "I {**TEAM CBB3**}, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
    "\n",
    "Non-compliance with the honour code constitutes a material breach of contract.\n",
    "\n",
    "### Predict Overview: EDSA - Climate Change Belief Analysis 2022\n",
    "\n",
    "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "With this context, EDSA is challenging you during the Classification Sprint with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.\n",
    "\n",
    "### Data overview\n",
    "Data The collection of this data was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43943 tweets were collected. Each tweet is labelled as one of the following classes:\n",
    "\n",
    "Class Description\n",
    "* 2 News: the tweet links to factual news about climate change\n",
    "* 1 Pro: the tweet supports the belief of man-made climate change\n",
    "* 0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "* -1 Anti: the tweet does not believe in man-made climate change\n",
    "\n",
    "Variable definitions\n",
    "- sentiment: Sentiment of tweet\n",
    "- message: Tweet body\n",
    "- tweetid: Twitter unique id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a12f436",
   "metadata": {},
   "source": [
    "# Team Supervisor\n",
    "- Chris Barnett\n",
    "\n",
    "# TEAM CBB3 MEMBERS\n",
    "- 1. Elelwani Tshikovhi (Team Leader);\n",
    "- 2. Katlego Maponya (Team coordinator) ;\n",
    "- 3. Musa Mashaba ;\n",
    "- 4. Zwothandwa Kunene ;\n",
    "- 5. Sinethemba Nongqoto;\n",
    "- 6. Desree Maleka\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ded8e9",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Data Engineering</a>\n",
    "\n",
    "<a href=#four>4. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438dc4e6",
   "metadata": {},
   "source": [
    "<a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "|  |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78070faf",
   "metadata": {},
   "source": [
    "# Import all the  **libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3812352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis and wrangling libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Visualisations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Preprocessing\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "# Modelling\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Metrics for Model Evaluation\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Downloads\n",
    "#nltk.download('all')\n",
    "#nltk.download('stopwords')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa75f48",
   "metadata": {},
   "source": [
    "### Download NLTK Corpora\n",
    "Some of the `nltk` text processing methods introduced in this train involve a lookup operation. For example, to find all [stopwords](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/) in a given string of text, we require a list of all possible stopwords in the English language to use for the lookup. Such a list is refered to as a [corpus](https://en.wikipedia.org/wiki/Text_corpus). Therefore, we need to first download the corpora we're going use, otherwise we may get a lookup error! Watch out specifically for the `tokenize` and `stopwords` sections. Not to worry, as we can easily avoid these errors by downloading the [corpora](http://www.nltk.org/nltk_data/) using the `nltk` downloader tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ffc51f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(['punkt','stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e5d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b20bef0",
   "metadata": {},
   "source": [
    "pip intall comet version control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96aabdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: comet_ml in c:\\users\\user\\anaconda3\\lib\\site-packages (3.31.3)\n",
      "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from comet_ml) (0.20.42)\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from comet_ml) (3.0.2)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from comet_ml) (1.12.1)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from comet_ml) (3.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from comet_ml) (1.16.0)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from comet_ml) (7.352.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from comet_ml) (0.9.1)\n",
      "Requirement already satisfied: websocket-client>=0.55.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from comet_ml) (1.3.2)\n",
      "Requirement already satisfied: semantic-version>=2.8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from comet_ml) (2.10.0)\n",
      "Requirement already satisfied: everett[ini]>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from comet_ml) (3.0.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from comet_ml) (2.26.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\lib\\site-packages (from dulwich!=0.20.33,>=0.20.6->comet_ml) (2021.10.8)\n",
      "Requirement already satisfied: urllib3>=1.24.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from dulwich!=0.20.33,>=0.20.6->comet_ml) (1.26.7)\n",
      "Requirement already satisfied: configobj in c:\\users\\user\\anaconda3\\lib\\site-packages (from everett[ini]>=1.0.1->comet_ml) (5.0.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (58.0.4)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (21.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.18.4->comet_ml) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.18.4->comet_ml) (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a8cea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/elelwani-tshikovhi/team-cbb-3-classification/496269c15d4944048231ee379e51f72a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import comet_ml at the top \n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Create an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key=\"rI7gAvhuv8lNvQcjSox3TjwIF\",\n",
    "    project_name=\"team-cbb-3-classification\",\n",
    "    workspace=\"elelwani-tshikovhi\",\n",
    ")\n",
    "\n",
    "# Run your code and go to /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd42284",
   "metadata": {},
   "source": [
    "*   [Trello Link](https://trello.com/b/2KvEPRJi/advanced-classificatin-team-cbb3-week-1)\n",
    "*   [Link to comet](https://www.comet.ml/elelwani-tshikovhi/team-cbb-3-classification/view/new/panels)\n",
    "*   [Link to Github](https://github.com/TEAMCBB3Classificationpredict)\n",
    "*   [Link to streamlit]( http://34.240.43.219:5000)\n",
    "*   [link to presentation](https://docs.google.com/presentation/d/19E1pbrp_lXfBG3FyOsz43QQRqq-qd7eGtp00w7nESCs/edit#slide=id.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed07070",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eef35b6",
   "metadata": {},
   "source": [
    "The training and testing data, trains_set and test_set respectively are loaded as Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "238720ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "train = pd.read_csv('https://raw.githubusercontent.com/TEAMCBB3Classificationpredict/datasets/main/train.csv')\n",
    "\n",
    "# Load test data\n",
    "test = pd.read_csv('https://raw.githubusercontent.com/TEAMCBB3Classificationpredict/datasets/main/test_with_no_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b902548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Worth a read whether you do or don't believe i...</td>\n",
       "      <td>425577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @thenation: Mike Pence doesn’t believe in g...</td>\n",
       "      <td>294933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @makeandmendlife: Six big things we can ALL...</td>\n",
       "      <td>992717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>@AceofSpadesHQ My 8yo nephew is inconsolable. ...</td>\n",
       "      <td>664510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @paigetweedy: no offense… but like… how do ...</td>\n",
       "      <td>260471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954\n",
       "5          1  Worth a read whether you do or don't believe i...   425577\n",
       "6          1  RT @thenation: Mike Pence doesn’t believe in g...   294933\n",
       "7          1  RT @makeandmendlife: Six big things we can ALL...   992717\n",
       "8          1  @AceofSpadesHQ My 8yo nephew is inconsolable. ...   664510\n",
       "9          1  RT @paigetweedy: no offense… but like… how do ...   260471"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing the top 10 values of the train data\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05e0b655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @nycjim: Trump muzzles employees of several...</td>\n",
       "      <td>75639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@bmastenbrook yes wrote that in 3rd yr Comp Sc...</td>\n",
       "      <td>211536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @climatehawk1: Indonesian farmers weather #...</td>\n",
       "      <td>569434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @guardian: British scientists face a ‘huge ...</td>\n",
       "      <td>315368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Aid For Agriculture | Sustainable agriculture ...</td>\n",
       "      <td>591733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid\n",
       "0  Europe will now be looking to China to make su...   169760\n",
       "1  Combine this with the polling of staffers re c...    35326\n",
       "2  The scary, unimpeachable evidence that climate...   224985\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928\n",
       "5  RT @nycjim: Trump muzzles employees of several...    75639\n",
       "6  @bmastenbrook yes wrote that in 3rd yr Comp Sc...   211536\n",
       "7  RT @climatehawk1: Indonesian farmers weather #...   569434\n",
       "8  RT @guardian: British scientists face a ‘huge ...   315368\n",
       "9  Aid For Agriculture | Sustainable agriculture ...   591733"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing the top 10 values of the test data\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b588ce7c",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "|This phase is important. This will help to understand patterns in the data, pinpoint any outliers and indicate relationships between variables uusing  descriptive statistics and data visualisations\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5e1eb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape : rows, columns: (15819, 3)\n",
      "Train shape : rows, columns: (10546, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Train shape : rows, columns:',train.shape)\n",
    "print('Train shape : rows, columns:',test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bdab4",
   "metadata": {},
   "source": [
    "On the train dataset we have 15819 rows and 3 columns while on the test we have 10546 rows and 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52518c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15819 entries, 0 to 15818\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   sentiment  15819 non-null  int64 \n",
      " 1   message    15819 non-null  object\n",
      " 2   tweetid    15819 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 370.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# checking the train data info and understanding the dataframe data types\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f600c0",
   "metadata": {},
   "source": [
    "From the above we can see that we do not have any nulls on train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97d37fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10546 entries, 0 to 10545\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   message  10546 non-null  object\n",
      " 1   tweetid  10546 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 164.9+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fb1979",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "This notebook trains a sentiment analysis model to classify climate change sentiments as Anti, Neutral, Pros, and News, based on the text of the news. \n",
    "\n",
    "We will split by 80% training dataset and 20% test dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25947fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split for  (train 80%, and test 20%)\n",
    "x_train, x_test, y_train, y_test = train_test_split(train['message'].tolist(),train['sentiment'].tolist(), test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46b81dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12638\n",
      "3160\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34f956c",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data cleaning\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data Cleaning ⚡ |\n",
    "| :--------------------------- |\n",
    "|  clean the dataset, and possibly create new features -using Natural language process . |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b60e6d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_url(text): \n",
    "    url_pattern  = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return url_pattern.sub(r'', text)\n",
    " # converting return value from list to string\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text ): \n",
    "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
    "    delete_dict[' '] = ' ' \n",
    "    table = str.maketrans(delete_dict)\n",
    "    text1 = text.translate(table)\n",
    "    #print('cleaned:'+text1)\n",
    "    textArr= text1.split()\n",
    "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>2))]) \n",
    "    \n",
    "    return text2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a0c747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Train data--------\n",
      "Anti = -1, Neutral = 0, Pros = 1, and News = 2\n",
      " 1    8529\n",
      " 2    3640\n",
      " 0    2334\n",
      "-1    1295\n",
      "Name: sentiment, dtype: int64\n",
      "15798\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "train.dropna(axis = 0, how ='any',inplace=True) \n",
    "train['Num_words_text'] = train['message'].apply(lambda x:len(str(x).split())) \n",
    "mask = train['Num_words_text'] >3\n",
    "train = train[mask]\n",
    "print('-------Train data--------')\n",
    "print('Anti = -1, Neutral = 0, Pros = 1, and News = 2')\n",
    "print(train['sentiment'].value_counts())\n",
    "print(len(train))\n",
    "print('-------------------------')\n",
    "max_train_sentence_length  = train['Num_words_text'].max()\n",
    "\n",
    "\n",
    "train['message'] = train['message'].apply(remove_emoji)\n",
    "train['message'] = train['message'].apply(remove_url)\n",
    "train['message'] = train['message'].apply(clean_text)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------\n",
    "\n",
    "train.dropna(axis = 0, how ='any',inplace=True) \n",
    "train['Num_words_text'] = train['message'].apply(lambda x:len(str(x).split())) \n",
    "mask = train['Num_words_text'] >3\n",
    "train = train[mask]\n",
    "print('-------Train data--------')\n",
    "print('Anti = -1, Neutral = 0, Pros = 1, and News = 2')\n",
    "print(train['sentiment'].value_counts())\n",
    "print(len(train))\n",
    "print('-------------------------')\n",
    "max_train_sentence_length  = train['Num_words_text'].max()\n",
    "\n",
    "\n",
    "train['message'] = train['message'].apply(remove_emoji)\n",
    "train['message'] = train['message'].apply(remove_url)\n",
    "train['message'] = train['message'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331778f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb2c5cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "595bf536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12638\n",
      "3160\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b82ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ad6a034",
   "metadata": {},
   "source": [
    "## Step 2: Text pre-processing\n",
    "\n",
    "Machine learning algorithms take numbers as inputs, not text, which means that we need to convert the texts into numerical vectors. We proceed as follows:\n",
    "\n",
    "### 1. Tokenization\n",
    "\n",
    "It consists in dividing the texts into words or smaller sub-texts, allowing us to determine the “vocabulary” of the dataset (set of unique tokens present in the data). Usually we use word-level representation. For our exemple we will use NLTK Tokenizer()\n",
    "\n",
    "### 2. Word indexing:\n",
    "\n",
    "Construct a vocablary_index mapper based on word frequency: the index would be inversely proportional to the word occurrence frequency in the overall dataset. the most frequent world would have index=1.. And every single word would get a unique index.\n",
    "\n",
    "These two steps are factorized as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913740a7",
   "metadata": {},
   "source": [
    "From this observation we can see that we have the highest number of ('Pros' = 8529) people that believe in the man-made climate change and 1295 (lowest) number of people that don't believe in the man-made climate change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618a6ed3",
   "metadata": {},
   "source": [
    "#Tokenize the sentences\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "#preparing vocabulary\n",
    "tokenizer.fit_on_texts(list(x_train))\n",
    "\n",
    "#converting text into integer sequences\n",
    "x_tr_seq  = tokenizer.texts_to_sequences(x_train) \n",
    "x_val_seq = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "#padding to prepare sequences of same length\n",
    "x_tr_seq  = pad_sequences(x_tr_seq, maxlen=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac8acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e1ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86a6be65",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, create one or more regression models that are able to accurately predict the Sentiment. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0198409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92345e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53fb7df1",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "## 6. Model Performance\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model performance ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section you are required to compare the relative performance of the various trained ML models on a holdout dataset and comment on what model is the best and why. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5592c0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66154507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddc1c80f",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "## 7. Model Explanations\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Model explanation ⚡ |\n",
    "| :--------------------------- |\n",
    "| In this section, you are required to discuss how the best performing model works in a simple way so that both technical and non-technical stakeholders can grasp the intuition behind the model's inner workings. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3513b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
